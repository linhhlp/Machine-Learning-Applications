{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39ce42a",
   "metadata": {},
   "source": [
    "# 4. Demo of Chatbot / OpenAI "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c61977",
   "metadata": {},
   "source": [
    "# Instroduction\n",
    "\n",
    "ChatGPT (Chat Generative Pre-Trained Transformer) is talking everywhere and so hot that big companies like Google also talk about its impact and strategy to resolve its influence.\n",
    "\n",
    "In this short demo, I will show how to use Python and its library to access API provided by OpenAI and use some popular/useful features. These features are provided via Endpoints and use different pre-trained models to answer any issues.\n",
    "\n",
    "## Model (was Engine) Endpoints\n",
    "\n",
    "There are a couple of models that can be used in different contexts/situations, hence, with different accuracy. For example, Davinci is the most capable model, and Ada is the fastest. While Davinci is generally the most capable, the other models can perform certain tasks extremely well with significant speed or cost advantages. For example, Curie can perform many of the same tasks as Davinci, but faster and for 1/10th the cost. [Read more here](https://platform.openai.com/docs/models/gpt-3)\n",
    "\n",
    "* List models\n",
    "\n",
    "GET https://api.openai.com/v1/models\n",
    "\n",
    "Lists the currently available models, and provides basic information about each one such as the owner and availability.\n",
    "\n",
    "\n",
    "## Install dependencies\n",
    "\n",
    "We can get \"openai\" library from conda\n",
    "\n",
    "```\n",
    "conda install -c conda-forge openai\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "* Data Limited knowledge of world and events after 2021\n",
    "\n",
    "* May occasionally generate incorrect information\n",
    "\n",
    "* May occasionally produce harmful instructions or biased content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e629642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-######\" # Insert your API here\n",
    "# get one free at https://openai.com/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319ac6c",
   "metadata": {},
   "source": [
    "# Completion Endpoints\n",
    "\n",
    "Based on an input, we can ask an engine to complete the sentence. There are several tasks we can ask, for example, ask a question, classify sentiment or a general caterogy, transformation (translation) or complete incomplete text.\n",
    "\n",
    "There are important paraments\n",
    "\n",
    "* model: Depending on speend, cost, accuracy, we can choose different models.\n",
    "\n",
    "* temperature: Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. Default value: 1\n",
    "\n",
    "* top_p: Nucleus sampling. 0.1 indicates that only the top 10% probability mass tokens will be evaluated. Default value: 1\n",
    "\n",
    "* max_tokens: maximum number of tokens to generate in the completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bbde98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def complete(\n",
    "    prompt,\n",
    "    suffix=None,\n",
    "    model=\"text-davinci-003\",\n",
    "    temperature=1,\n",
    "    max_tokens=60,\n",
    "    top_p=1,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "):\n",
    "    try:\n",
    "        response = openai.Completion.create(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p,\n",
    "            suffix=suffix,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "        )\n",
    "        return response.choices\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "912f69d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Lionel Messi is an Argentinian professional footballer who plays as a forward for Spanish club Barcelona and the Argentina national team. Messi is widely considered to be one of the greatest players of all time. He is the first player in history to win five FIFA Ballon d'Or awards,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me about Messi.\"\n",
    "r = complete(prompt, model=\"text-davinci-003\", temperature=0.9)\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f42c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I think Messi is a phenomenal footballer, and I greatly admire his skill and determination. He\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How do you like Messi.\"\n",
    "r = complete(prompt, model=\"text-davinci-003\", temperature=0.9, max_tokens=20)\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a08bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt text:\n",
      "Classify the Text's label as positive, neutral, or negative.\n",
      "Text: I loved the football!\n",
      "Sentiment:\n",
      "\n",
      " Positive\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Classify the Text's label as positive, neutral, or negative.\\n\"\n",
    "    \"Text: I loved the football!\\n\"\n",
    "    \"Sentiment:\"\n",
    ")\n",
    "\n",
    "r = complete(prompt, model=\"text-davinci-003\", temperature=0.9)\n",
    "print(\"Prompt text:\\n\" + prompt + \"\\n\")\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f56293",
   "metadata": {},
   "source": [
    "### Temperature Impact\n",
    "Let's see how temperature change the text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8372106b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt text:\n",
      "Classify the Text.\n",
      " Text: Today is so rainy and I love raining!\n",
      "\n",
      "\n",
      "Temperature = 1: \n",
      "The text is classified as weather.\n",
      "Temperature = 0.75: \n",
      "The text is about the weather.\n",
      "Temperature = 0.5: \n",
      "The text is classified as a description of the weather.\n",
      "Temperature = 0.25: \n",
      "The text is a description of the weather.\n",
      "Temperature = 0: \n",
      "The text is about the weather.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Classify the Text.\\n Text: Today is so rainy and I love raining!\\n\"\n",
    "print(\"Prompt text:\\n\" + prompt + \"\\n\")\n",
    "temp = [1, 0.75, 0.5, 0.25, 0]\n",
    "for t in temp:\n",
    "    r = complete(prompt, model=\"text-davinci-002\", temperature=t)\n",
    "    if r:\n",
    "        print(\"Temperature = \" + str(t) + \": \" + r[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929ff1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt text:\n",
      "Classify the sentiment in these tweets:\n",
      "1. \"I enjoy this program\"\n",
      "2. \"I don't like him anymore\"\n",
      "3. \"I can't wait for you\"\n",
      "4. \"I can't wait for you anymore\"\n",
      "5. \"I can't wait for Halloween!!!\"\n",
      "6. \"I do not hate much jokers\"\n",
      "7. \"I love eating chocolates <3!\"\n",
      "Tweet sentiment ratings:\n",
      "\n",
      "\n",
      "1. Positive \n",
      "2. Negative \n",
      "3. Positive \n",
      "4. Negative \n",
      "5. Positive \n",
      "6. Positive \n",
      "7. Positive\n"
     ]
    }
   ],
   "source": [
    "# Multiple requirements at once\n",
    "prompt = (\n",
    "    \"Classify the sentiment in these tweets:\\n\"\n",
    "    '1. \"I enjoy this program\"\\n'\n",
    "    '2. \"I don\\'t like him anymore\"\\n'\n",
    "    '3. \"I can\\'t wait for you\"\\n'\n",
    "    '4. \"I can\\'t wait for you anymore\"\\n'\n",
    "    '5. \"I can\\'t wait for Halloween!!!\"\\n'\n",
    "    '6. \"I do not hate much jokers\"\\n'\n",
    "    '7. \"I love eating chocolates <3!\"\\n'\n",
    "    \"Tweet sentiment ratings:\"\n",
    ")\n",
    "\n",
    "r = complete(prompt, model=\"text-davinci-003\", temperature=0.9)\n",
    "print(\"Prompt text:\\n\" + prompt + \"\\n\")\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0472361e",
   "metadata": {},
   "source": [
    "### Suffix\n",
    "\n",
    "The suffix that comes after a completion of inserted text to helps the model for a better text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "800eb0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt text:\n",
      "Messi scored multiple goals in World Cup. Before that, he played well in all soccer clubs. He helps his colleague to score as well.\n",
      " Messi is a fantastic player with amazing skills. \n",
      "Messi is the greatest of all the time player.(suffix)\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Messi scored multiple goals in World Cup. Before that, he played\"\n",
    "    \" well in all soccer clubs. He helps his colleague to score as well.\"\n",
    ")\n",
    "suffix = \"Messi is the greatest of all the time player.\"\n",
    "r = complete(\n",
    "    prompt,\n",
    "    suffix=suffix,\n",
    "    model=\"text-davinci-003\",\n",
    "    temperature=1,\n",
    "    max_tokens=260,\n",
    ")\n",
    "print(\"Prompt text:\\n\" + prompt)\n",
    "if r:\n",
    "    print(r[0].text)\n",
    "print(suffix + \"(suffix)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa135b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt text:\n",
      "How to lose weight:\n",
      "1. Do not skip breakfast.\n",
      "\n",
      "2. Eat small meals at regular intervals throughout the day.\n",
      "3. Eat fewer fatty and sugary foods.\n",
      "4. Eat more fruits and vegetables.\n",
      "5. Exercise regularly.\n",
      "6. Drink plenty of water.\n",
      "7. Limit your alcohol consumption.\n",
      "8. Reduce your intake of processed foods.\n",
      "9. Avoid snacking on unhealthy foods.\n",
      "10. Get plenty of sleep.\n",
      "11. Manage your stress levels.\n",
      "\n",
      "\n",
      "12. Plan your meals(suffix)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to lose weight:\\n1. Do not skip breakfast.\"\n",
    "suffix = \"12. Plan your meals\"\n",
    "r = complete(\n",
    "    prompt,\n",
    "    suffix=suffix,\n",
    "    model=\"text-davinci-003\",\n",
    "    temperature=1,\n",
    "    max_tokens=260,\n",
    ")\n",
    "print(\"Prompt text:\\n\" + prompt)\n",
    "if r:\n",
    "    print(r[0].text)\n",
    "print(suffix + \"(suffix)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdbfa40",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "The model can summarize a long paragraph. The new length could be alterred in \"max_tokens\"\n",
    "\n",
    "* max_tokens: Set small value if we want short summary\n",
    "\n",
    "* frequency_penalty: Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Number between -2.0 and 2.0\n",
    "\n",
    "* presence_penalty: Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. Number between -2.0 and 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bce30c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text:\n",
      "One person is in hospital and others are without a home in the aftermath of a Christmas evening fire at a fourplex in southeast Calgary.  The Calgary Fire Department was called to the 2800 block of 15 Avenue S.E. around 8 p.m. on Sunday, Dec. 25.  Firefighters doused flames in the basement of the fourplex, keeping them from spreading beyond the suite they started in.  CFD found and rescued one resident, who was taken to hospital in critical condition.  Other residents of the fourplex home at the time of the fire were able to escape on their own.  The fire department says smoke damage inside one of the upper suites resulted in the displacement of some residents.  Investigation into the cause of the fire is ongoing.\n",
      "Summary:\n",
      "\n",
      "\n",
      "- one person in hospital, critical condition\n",
      "- others displaced\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"One person is in hospital and others are without a home in the \"\n",
    "    \"aftermath of a Christmas evening fire at a fourplex in southeast \"\n",
    "    \"Calgary.  The Calgary Fire Department was called to the 2800 block \"\n",
    "    \"of 15 Avenue S.E. around 8 p.m. on Sunday, Dec. 25.  Firefighters \"\n",
    "    \"doused flames in the basement of the fourplex, keeping them from \"\n",
    "    \"spreading beyond the suite they started in.  CFD found and rescued \"\n",
    "    \"one resident, who was taken to hospital in critical condition.  \"\n",
    "    \"Other residents of the fourplex home at the time of the fire were \"\n",
    "    \"able to escape on their own.  The fire department says smoke damage \"\n",
    "    \"inside one of the upper suites resulted in the displacement of some \"\n",
    "    \"residents.  Investigation into the cause of the fire is ongoing.\"\n",
    ")\n",
    "print(\"Full text:\\n\" + prompt)\n",
    "print(\"Summary:\")\n",
    "r = complete(\n",
    "    prompt,\n",
    "    model=\"text-davinci-002\",\n",
    "    temperature=1,\n",
    "    max_tokens=32,\n",
    "    frequency_penalty=1,\n",
    "    presence_penalty=1,\n",
    ")\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400569c",
   "metadata": {},
   "source": [
    "## Translation: \n",
    "\n",
    "We can translate even into multiple languages at once. We need to set \"temperature\" to 0 to get the most accurate translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7b0975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Le chatbot n'est pas quelque chose de nouveau, mais il y a beaucoup de possibilités d'amélioration.\n",
      "2. El chatbot no es algo nuevo, pero hay mucho espacio para mejorar.\n",
      "3. チャットボットは新しいものではありませんが、改\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Translate this into 1. French, 2. Spanish and 3. Japanese.\\n\"\n",
    "    \"Chatbot is not something new, but there are huge room to improve.\"\n",
    ")\n",
    "r = complete(prompt, model=\"text-davinci-003\", temperature=0, max_tokens=100)\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a256641",
   "metadata": {},
   "source": [
    "## Write about a Subject\n",
    "\n",
    "It is not an problem to ask the machine to write something or even tell a story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74361f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The future of Natural Language Processing with machine learning looks very promising. NLP is being used to solve some of the most challenging problems in marketing, healthcare, education, finance, and more. With more advances in NLP algorithms, machines will be able to understand and interact with humans in natural language more effectively. Furthermore, machine learning algorithms will be increasingly used to create better language models, improve accuracy in classifications, and also build better language-based applications. All these advances will ultimately lead to more efficient and powerful language processing applications.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Briefly summarize about future of Natural Language Processing\"\n",
    "    \" with machine learning.\"\n",
    ")\n",
    "r = complete(prompt, model=\"text-davinci-003\", temperature=1, max_tokens=128)\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99ba6874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "One day, there was a boy who found a fish in a lake. He was so excited to have found such a prize, and he took the fish home to show his mother. His mother was less than thrilled, and she told him to put the fish back in the lake. The boy was so disappointed, but he did as his mother said. When he got back to the lake, the fish was gone.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me a fun story but not happy ending.\"\n",
    "r = complete(prompt, model=\"text-davinci-002\", temperature=1, max_tokens=256)\n",
    "if r: print(r[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d58ec6",
   "metadata": {},
   "source": [
    "# Edit Text / Code\n",
    "\n",
    "OpenAI’s GPT-3 model can edit text, which has various applications like grammar and punctuation correction, rewriting text, or any other simple instructions that can be performed on a text.\n",
    "\n",
    "## Parameters\n",
    "\n",
    "* model: ID of the model to use. You can use the text-davinci-edit-001 or code-davinci-edit-001\n",
    "\n",
    "* input: The input text to use as a starting point for the edit. Defaults to ''\n",
    "\n",
    "* instruction (Required): The instruction that tells the model how to edit the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "815dfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit(\n",
    "    instruction,\n",
    "    input=\"\",\n",
    "    model=\"text-davinci-edit-001\",\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "):\n",
    "    try:\n",
    "        response = openai.Edit.create(\n",
    "            model=model,\n",
    "            instruction=instruction,\n",
    "            input=input,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        return response.choices\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00aa0351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What day of the week is it today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"What dayy of the wek is its?\"\n",
    "instruction = \"Fix the spelling mistakes and grammar\"\n",
    "r = edit(instruction=instruction, input=text, model=\"text-davinci-edit-001\")\n",
    "if r: \n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ecb440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def trySomething(sefl, go):\n",
      "    print(\"Good job\")\n",
      "    if go == \"1\":\n",
      "        print(\"Nan\")\n",
      "    return a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "def trySomething(sefl, go):\n",
    "    print(\"Good job\")\n",
    "     if go == \"1\"\n",
    "     print(\"Nan\")\n",
    "    return a\n",
    "\"\"\"\n",
    "instruction = \"Fix the code in Python\"\n",
    "r = edit(\n",
    "    instruction=instruction,\n",
    "    input=text,\n",
    "    model=\"code-davinci-edit-001\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "if r:\n",
    "    print(r[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e971d8",
   "metadata": {},
   "source": [
    "# Other Usages\n",
    "\n",
    "## Image Generation\n",
    "\n",
    "* Creating images from scratch based on a text prompt\n",
    "* Creating edits of an existing image based on a new text prompt\n",
    "* Creating variations of an existing image\n",
    "\n",
    "[More](https://platform.openai.com/docs/guides/images/introduction)\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. Embeddings are commonly used for:\n",
    "\n",
    "* Search (where results are ranked by relevance to a query string)\n",
    "* Clustering (where text strings are grouped by similarity)\n",
    "* Recommendations (where items with related text strings are recommended)\n",
    "* Anomaly detection (where outliers with little relatedness are identified)\n",
    "* Diversity measurement (where similarity distributions are analyzed)\n",
    "* Classification (where text strings are classified by their most similar label)\n",
    "\n",
    "[More] (https://platform.openai.com/docs/guides/embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab7c3f",
   "metadata": {},
   "source": [
    "# More resources\n",
    "\n",
    "0. https://www.educative.io/courses/open-ai-api-natural-language-processing-python/\n",
    "\n",
    "1. https://platform.openai.com/docs/models\n",
    "\n",
    "2. https://platform.openai.com/docs/api-reference/completions\n",
    "\n",
    "3. https://help.openai.com/en/articles/5832130-what-s-changed-with-engine-names-and-best-practices\n",
    "\n",
    "4. https://jimmymwhitaker.medium.com/openais-chatgpt-19e7fd58ff73\n",
    "\n",
    "5. https://medium.datadriveninvestor.com/openai-quietly-released-gpt-3-5-heres-what-you-can-do-with-it-4dee22aea438"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
