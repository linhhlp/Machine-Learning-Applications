{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd53c25",
   "metadata": {},
   "source": [
    "# 1. Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675ea04",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450f0f0",
   "metadata": {},
   "source": [
    "Sequence to Sequence (seq2seq) Model is one of the most important frameworks in NLP which includes many applications like chatbots (Text generators), text summarization, and translators.\n",
    "\n",
    "Recently, seq2seq is empowered with Machine Learning in both encoder and decoder models. These models are both language models and are often built with LSTM (Long short-term memory) models.\n",
    "\n",
    "First, text input is fed to the encoder (sequence), the output of this is fed to the decoder (to), and the final output is transformed back to the text (sequence).\n",
    "\n",
    "Because this task is sequence-based, we can use Recurrent Neural Networks (RNNs) or Gated Recurrent Units (GRU) (computationally more efficient than LSTM) depending on the design of the system and accuracy requirement.\n",
    "\n",
    "![Demo](https://miro.medium.com/max/828/1*_6-EVV3RJXD5KDjdnxztzg@2x.webp)\n",
    "[Image source and read more here](https://medium.com/@Aj.Cheng/seq2seq-18a0730d1d77)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482f801",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a0cc2",
   "metadata": {},
   "source": [
    "### Training Task\n",
    "\n",
    "There are two tasks during training:\n",
    "\n",
    "1. Input Task: given an input sequence (text) and extract useful information\n",
    "2. Output Task: Built weight parameters in neural networks and calculate word probabilities from both input sequence and previous words in the output sequence (known output text).\n",
    "\n",
    "However, there are couples of issues while dealing with languages.\n",
    "1. Standardize Unicode letters and/or convert them to ASCII to simplify the process.\n",
    "2. Data Cleaning is very important to remove not related information as well as special symbols, and letters. If data was crawled from websites, HTML tags must be checked to remove them.\n",
    "3. The length output is not given/fixed, such as translation, and summarization of text. But the input of the model is fixed when building neural networks. An extra symbol was filled into an empty slot called a pad.\n",
    "4. The input/output is not required, but we need Machine returns something. So we use SOS and EOS (start-of-sequence and end-of-sequence) tokens.\n",
    "5. Tokenization is important in some languages. For example, in Vietnamese, a unit word can consist of two or more separate words)\n",
    "6. We also want to build stop-words that repeat many times but have less meaning (such as \"and\", \"or\", and \"of\"). In a subject-oriented, for example, in the subject of air transport business, \"airplane\" should be a stop-word.\n",
    "\n",
    "These extra symbols are called new vocabulary or extended vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213baab",
   "metadata": {},
   "source": [
    "### ATTENTION \n",
    "\n",
    "In the encoder-decoder model, because the decoder gets information from the final state in each layer of an encoder, it is hard to capture all of the informative data in case of large input which also might contain long-term dependencies. \n",
    "\n",
    "The solution is to let the decoder also access to intermediate time step using ATTENTION which calculates *context vector* at each time step. Attention can be another neural network. By tuning this attention, we can get a great result for long paragraph input.\n",
    "\n",
    "\n",
    "![Attention - source educative.io](attention.png)\n",
    "*\\(source educative.io)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
