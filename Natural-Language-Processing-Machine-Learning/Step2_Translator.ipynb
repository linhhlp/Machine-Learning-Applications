{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd53c25",
   "metadata": {},
   "source": [
    "# 2. Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675ea04",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f450f0f0",
   "metadata": {},
   "source": [
    "In this example, I will build LSTM single-layer neural networks for both the encoder and decoder. We need to take care of the final forward and backward states from a single layer to the decoder.\n",
    "\n",
    "```python\n",
    "# LSTM layer in Encoder\n",
    "lstm_layer = tf.keras.layers.LSTM(\n",
    "    units,  # dimensionality of the output space\n",
    "    return_sequences=True,  # Pass output sequence and state to Decoder\n",
    "    return_state=True,\n",
    ")\n",
    "```\n",
    "However, we can improve the accuracy by implementing BiLSTM or multi-layer LSTM/BiLSTM. Let's create a BiLSTM model with forward and backward layers:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "forward_layer = tf.keras.layers.LSTM(10, return_sequences=True)\n",
    "backward_layer = tf.keras.layers.LSTM(\n",
    "    10, activation=\"relu\", return_sequences=True, go_backwards=True\n",
    ")\n",
    "model.add(\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        forward_layer, backward_layer=backward_layer, input_shape=(5, 10)\n",
    "    )\n",
    ")\n",
    "model.add(Dense(5))\n",
    "model.add(Activation(\"softmax\"))\n",
    "```\n",
    "\n",
    "There is a tutorial to build [Encoder-Decoder Model using LSTM](https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/) and [compare LSTM with BiLSTM](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482f801",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "975b72ab",
   "metadata": {},
   "source": [
    "### Training Task\n",
    "\n",
    "There are two tasks during training:\n",
    "\n",
    "1. Input Task: given an input sequence (text) and extract useful information\n",
    "2. Output Task: we need to process the output properly to calculate the probability. So that we need Ground Truth Sequence as the given information and Final Token Sequence as a result which model should predict when giving the Ground Truth Sequence.\n",
    "\n",
    "```python\n",
    "dec_input = targ[ : , :-1 ]   # Ground Truth Sequence\n",
    "real = targ[ : , 1: ]         # Final Token Sequence\n",
    "pred = decoder(dec_input, decoder_initial_state)\n",
    "logits = pred.rnn_output\n",
    "loss = loss_function(real, logits)\n",
    "```\n",
    "\n",
    "#### Data cleaning\n",
    "\n",
    "Standardize Unicode letters and convert to ASCII to simplify the process. \n",
    "*unicodedata.normalize(form, unistr)* :This function returns the normal form for the Unicode string unistr. Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.\n",
    "*unicodedata.category Mn* : Ignore NonSpacing Mark\n",
    "\n",
    "```python\n",
    "def unicode_to_ascii(s):\n",
    "    return \"\".join(\n",
    "        c\n",
    "        for c in unicodedata.normalize(\"NFD\", s)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "```\n",
    "\n",
    "Below is a sample code how to deal with special letters \n",
    "\n",
    "\n",
    "```python\n",
    "w = unicode_to_ascii(w.lower().strip())\n",
    "# creating a space between a word and the punctuation following it\n",
    "w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "w = re.sub(r'[\" \"]+', \" \", w)\n",
    "# replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "w = w.strip()\n",
    "```\n",
    "\n",
    "#### Padding\n",
    "The length input/output is not given / fixed, such as translation, summarization of text. But the input of model is fixed when building neural networks. An extra symbol was filled into empty space called pad.\n",
    "```python\n",
    "tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "```\n",
    "\n",
    "#### Start and End of a Sentence\n",
    "The output is not required, but we need Machine returns something. So we use start-of-sequence \\<start> and end-of-sequence \\<end> tokens.\n",
    "```python\n",
    "w = '<start> ' + w + ' <end>'\n",
    "```\n",
    "\n",
    "#### Out of Vocabulary\n",
    "There are special words which do not exist in dictionary, we introduce Out-Of-Vocabulary (OOV) token.\n",
    "```python\n",
    "tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "```\n",
    "\n",
    "These extra symbols called new vocabulary or extended vocabulary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1cbd420",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "There are two popular Attentions developed by Bahdanau (tfa.seq2seq.BahdanauAttention) and Luong (tfa.seq2seq.LuongAttention).\n",
    "Although the idea to use attention is easy to understand, implementation is complex. Fortunately, there is a helper in TensorFlow *AttentionWrapper* to add attention to the decoder cell.\n",
    "\n",
    "```python\n",
    "# Luong Attention\n",
    "attention_mechanism = tfa.seq2seq.LuongAttention(\n",
    "    dec_units, memory, memory_sequence_length\n",
    ")\n",
    "rnn_cell = tfa.seq2seq.AttentionWrapper(\n",
    "    tf.keras.layers.LSTMCell,\n",
    "    attention_mechanism,\n",
    "    attention_layer_size=dec_units,\n",
    ")\n",
    " \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64f89622",
   "metadata": {},
   "source": [
    "### Decoding during Training\n",
    "\n",
    "During training, we have access to both the input and output sequences of a training pair. This means that we can use the output sequence's ground truth tokens as input for the decoder.\n",
    "\n",
    "The TrainingSampler object is initialized with the (embedded) ground truth sequences and the lengths of the ground truth sequences.\n",
    "\n",
    "```python\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "decoder = tfa.seq2seq.BasicDecoder(rnn_cell, sampler=sampler, output_layer=fc)\n",
    "```\n",
    "\n",
    "### Decoding during Inferencing\n",
    "\n",
    "When inferencing, there is no ground truth. Hence, we need to change TrainingSampler object to an inference helper. In this example, I show BasicDecoder from tf-addons which uses GreedyEmbeddingSampler. There is another helper [BeamSearchDecoder also from tf-addons](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt#use_tf-addons_beamsearchdecoder).\n",
    "\n",
    "```python\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(\n",
    "    cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda1bdd",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "I will build a Translator from Vietnamese to English. The data was downloaded from http://www.manythings.org/anki/ and pre-processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e10ee35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "\n",
    "from nmtdataset import NMTDataset\n",
    "from models import Encoder, Decoder\n",
    "from functions import *\n",
    "\n",
    "def get_nmt():\n",
    "    \"\"\"Get the link to the dataset.\n",
    "    If the dataset does not exist, download it manually and assign new path.\"\"\"\n",
    "    path_to_file = \"./dict/vie-eng/vie.txt\"\n",
    "    return path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc3d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "# DataSet\n",
    "BUFFER_SIZE = 256000\n",
    "BATCH_SIZE = 64\n",
    "num_examples = 10000 # Let's limit the #training examples for faster training\n",
    "# Neural Network parameters\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = num_examples//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a8e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataSet\n",
    "dataset_creator = NMTDataset(\"en-vie\", get_nmt())\n",
    "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(\n",
    "    num_examples, BUFFER_SIZE, BATCH_SIZE\n",
    ")\n",
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e738326c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Encoder Stack\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b729285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test decoder stack\n",
    "decoder = Decoder(\n",
    "    vocab_tar_size,\n",
    "    embedding_dim,\n",
    "    units,\n",
    "    BATCH_SIZE,\n",
    "    max_length_input,\n",
    "    max_length_output,\n",
    "    \"luong\",\n",
    ")\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(\n",
    "    BATCH_SIZE, [sample_h, sample_c], tf.float32\n",
    ")\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d137497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 0.8116 taken time  29.86 sec\n",
      "Epoch 2 Loss 0.6467 taken time  20.18 sec\n",
      "Epoch 3 Loss 0.5686 taken time  20.33 sec\n",
      "Epoch 4 Loss 0.5143 taken time  19.90 sec\n",
      "Epoch 5 Loss 0.4593 taken time  19.97 sec\n",
      "Epoch 6 Loss 0.4025 taken time  20.08 sec\n",
      "Epoch 7 Loss 0.3499 taken time  20.31 sec\n",
      "Epoch 8 Loss 0.2995 taken time  21.75 sec\n",
      "Epoch 9 Loss 0.2519 taken time  20.70 sec\n",
      "Epoch 10 Loss 0.2120 taken time  20.61 sec\n",
      "Epoch 11 Loss 0.1732 taken time  21.00 sec\n",
      "Epoch 12 Loss 0.1438 taken time  20.89 sec\n",
      "Epoch 13 Loss 0.1228 taken time  20.99 sec\n",
      "Epoch 14 Loss 0.1014 taken time  20.91 sec\n",
      "Epoch 15 Loss 0.0870 taken time  21.12 sec\n",
      "Epoch 16 Loss 0.0779 taken time  20.73 sec\n",
      "Epoch 17 Loss 0.0680 taken time  21.22 sec\n",
      "Epoch 18 Loss 0.0620 taken time  20.49 sec\n",
      "Epoch 19 Loss 0.0560 taken time  20.49 sec\n",
      "Epoch 20 Loss 0.0532 taken time  20.68 sec\n",
      "Epoch 21 Loss 0.0522 taken time  21.97 sec\n",
      "Epoch 22 Loss 0.0592 taken time  20.50 sec\n",
      "Epoch 23 Loss 0.0653 taken time  20.55 sec\n",
      "Epoch 24 Loss 0.0830 taken time  20.45 sec\n",
      "Epoch 25 Loss 0.0775 taken time  20.47 sec\n",
      "Epoch 26 Loss 0.0798 taken time  20.49 sec\n",
      "Epoch 27 Loss 0.0848 taken time  20.52 sec\n",
      "Epoch 28 Loss 0.0816 taken time  20.45 sec\n",
      "Epoch 29 Loss 0.0800 taken time  20.68 sec\n",
      "Epoch 30 Loss 0.0943 taken time  20.67 sec\n",
      "Epoch 31 Loss 0.3646 taken time  20.60 sec\n",
      "Epoch 32 Loss 1.2448 taken time  20.56 sec\n",
      "Epoch 33 Loss 0.9566 taken time  20.57 sec\n",
      "Epoch 34 Loss 0.6192 taken time  20.57 sec\n",
      "Epoch 35 Loss 0.5265 taken time  20.64 sec\n",
      "Epoch 36 Loss 0.4774 taken time  20.55 sec\n",
      "Epoch 37 Loss 0.4399 taken time  20.57 sec\n",
      "Epoch 38 Loss 0.4089 taken time  20.72 sec\n",
      "Epoch 39 Loss 0.3816 taken time  20.55 sec\n",
      "Epoch 40 Loss 0.3570 taken time  20.94 sec\n",
      "Epoch 41 Loss 0.3349 taken time  20.29 sec\n",
      "Epoch 42 Loss 0.3162 taken time  20.25 sec\n",
      "Epoch 43 Loss 0.3008 taken time  20.21 sec\n",
      "Epoch 44 Loss 0.2867 taken time  20.28 sec\n",
      "Epoch 45 Loss 0.2729 taken time  20.21 sec\n",
      "Epoch 46 Loss 0.2549 taken time  20.25 sec\n",
      "Epoch 47 Loss 0.2403 taken time  20.21 sec\n",
      "Epoch 48 Loss 0.2288 taken time  20.20 sec\n",
      "Epoch 49 Loss 0.2193 taken time  20.90 sec\n",
      "Epoch 50 Loss 0.2084 taken time  20.22 sec\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(\n",
    "            inp, targ, enc_hidden, BATCH_SIZE, encoder, decoder\n",
    "        )\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(\n",
    "        \"Epoch {} Loss {:.4f} taken time  {:.2f} sec\".format(\n",
    "            epoch + 1, total_loss / steps_per_epoch, time.time() - start\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db150d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result = evaluate_sentence(\n",
    "        dataset_creator.preprocess_sentence(sentence),\n",
    "        inp_lang,\n",
    "        targ_lang,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        max_length_input,\n",
    "        units,\n",
    "    )\n",
    "    print(result)\n",
    "    result = targ_lang.sequences_to_texts(\n",
    "        result\n",
    "    )  # Transform vertor numbers to words\n",
    "    print(\"Input: %s\" % (sentence))\n",
    "    print(\"Translation: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e20659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5  41 860  39  22   4   3]]\n",
      "Input: Tôi thích hoa.\n",
      "Translation: ['i like bread with me . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Tôi thích hoa.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a6fb0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  17   16  919 1345    4    3]]\n",
      "Input: Trời nắng.\n",
      "Translation: ['it s likely snowing . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Trời nắng.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe31a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 15 150  22   4   3]]\n",
      "Input: Anh yêu em.\n",
      "Translation: ['he love me . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Anh yêu em.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b38b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176 123   4   3]]\n",
      "Input: Tiếp tục đi.\n",
      "Translation: ['keep last . <end>']\n"
     ]
    }
   ],
   "source": [
    "translate(u'Tiếp tục đi.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
